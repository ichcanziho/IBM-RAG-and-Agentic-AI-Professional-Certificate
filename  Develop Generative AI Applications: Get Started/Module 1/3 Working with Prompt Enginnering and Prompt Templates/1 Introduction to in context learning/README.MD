# Clase: Introducci√≥n al In-context Learning y Prompt Engineering

Bienvenido a la introducci√≥n al **In-context Learning**. En este video, aprender√°s a describir este m√©todo y los fundamentos de la ingenier√≠a de prompts.

![img.png](img.png)
---
El **In-context Learning** es un m√©todo espec√≠fico de ingenier√≠a de prompts donde se proporcionan demostraciones de la tarea al modelo como parte del prompt en lenguaje natural. No requiere entrenamiento adicional; la tarea se aprende a partir de ejemplos presentados en el contexto durante el tiempo de inferencia.

![img_1.png](img_1.png)

---

### Ventajas y Desventajas

Este m√©todo no requiere que el modelo sea ajustado (*fine-tuned*) en conjuntos de datos espec√≠ficos, lo que reduce dr√°sticamente los recursos y el tiempo necesarios para adaptar los LLMs.

![img_2.png](img_2.png)

Sin embargo, tiene limitaciones: est√° restringido por lo que puede caber en el contexto. Las tareas muy complejas podr√≠an seguir requiriendo pasos de gradiente o enfoques de aprendizaje autom√°tico tradicionales que ajusten los pesos del modelo.

---

### ¬øQu√© es un Prompt?

Los **prompts** son instrucciones o entradas dadas a un LLM dise√±adas para guiarlo a realizar una tarea espec√≠fica. Tienen dos componentes principales: **Instrucciones** (comandos directos) y **Contexto** (informaci√≥n de fondo).

![img_3.png](img_3.png)

Al combinar estos elementos, puedes adaptar modelos de IBM, OpenAI, Google o Meta para tareas que van desde responder consultas hasta generar contenido complejo.

---

### Definici√≥n de Prompt Engineering

La **Ingenier√≠a de Prompts** es el proceso especializado de dise√±ar y refinar las preguntas o comandos para interactuar con sistemas de IA.

![img_4.png](img_4.png)

El objetivo no es solo preguntar, sino hacerlo de la mejor manera posible, creando prompts claros y ricos en contexto para obtener las respuestas m√°s precisas. Es fundamental en campos como la automatizaci√≥n del servicio al cliente y la ling√º√≠stica computacional.

---

### Importancia de la Ingenier√≠a de Prompts

Este proceso es crucial porque mejora la efectividad y precisi√≥n de los LLMs, asegura la relevancia de las respuestas y reduce los malentendidos al cumplir mejor las expectativas del usuario.

![img_5.png](img_5.png)

Adem√°s, permite que el modelo se adapte y aprenda dentro de su propio contexto, eliminando la necesidad de un re-entrenamiento constante.

---

### Ejemplo Pr√°ctico

Aqu√≠ vemos un ejemplo simple con GPT-3.5. Al proveer el inicio: *"The wind is..."*, el modelo genera una respuesta po√©tica y detallada.

![img_6.png](img_6.png)

Esto demuestra c√≥mo un prompt abierto gu√≠a al LLM para crear contenido creativo y atractivo, resaltando su capacidad de generaci√≥n.

---

### Anatom√≠a de un Prompt Estructurado (1/2)

Desglosemos los componentes de un prompt bien estructurado. Primero tenemos las **Instrucciones** y el **Contexto**.

![img_7.png](img_7.png)

* **Instrucci√≥n:** "Clasifica la rese√±a en neutral, negativa o positiva".
* **Contexto:** Indica que la rese√±a es parte del feedback de un producto reci√©n lanzado, lo que ayuda al modelo a ponderar el an√°lisis.

---

### Anatom√≠a de un Prompt Estructurado (2/2)

Los siguientes componentes son los **Datos de Entrada** y el **Indicador de Salida**.

![img_8.png](img_8.png)

* **Input Data:** Es el texto real a procesar (ej: "El producto lleg√≥ tarde pero la calidad excedi√≥ mis expectativas").
* **Output Indicator:** Es el marcador final (ej: "Sentiment:") que le indica a la IA d√≥nde debe entregar su an√°lisis.

---

### Resumen de Conceptos Clave

En resumen, los prompts consisten en instrucciones y contexto, y la ingenier√≠a de prompts es el arte de refinarlos para obtener resultados √≥ptimos.

![img_9.png](img_9.png)

> üí° **Nota de Gemini:** > Un truco de experto es usar delimitadores (como `###` o `"""`) para separar claramente las **Instrucciones** de los **Datos de Entrada**. Esto ayuda al modelo a no confundir la orden con el contenido que debe analizar.

---

### Conclusi√≥n del M√≥dulo

Has aprendido que el In-context Learning permite al modelo adaptarse sin fine-tuning y que un prompt robusto se basa en cuatro elementos: instrucciones, contexto, datos e indicador de salida.

